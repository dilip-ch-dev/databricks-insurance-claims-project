{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd8089e-3ed3-4246-af63-d615e25387cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set working catalog and schema\n",
    "spark.sql(\"USE CATALOG smart_claims_dev\")\n",
    "spark.sql(\"USE SCHEMA bronze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe0a15f-e7ca-4da4-968a-7579bf5eeb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting working catalog and schema\n",
    "spark.sql(\"USE CATALOG smart_claims_dev\")\n",
    "spark.sql(\"USE SCHEMA bronze\")\n",
    "\n",
    "print(f\"Current Catalog: {spark.catalog.currentCatalog()}\")\n",
    "print(f\"Current Database: {spark.catalog.currentDatabase()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9b2e1a-b61c-4263-ba30-b3475e136d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV sources \n",
    "csv_sources = {\n",
    "    \"claims_raw\": \"/Volumes/smart_claims_dev/landing/raw_files/sql_server/claims.csv\",\n",
    "    \"customers_raw\": \"/Volumes/smart_claims_dev/landing/raw_files/sql_server/customers.csv\",\n",
    "    \"policies_raw\": \"/Volumes/smart_claims_dev/landing/raw_files/sql_server/policies.csv\"\n",
    "}\n",
    "\n",
    "print(\"CSV sources configured:\")\n",
    "for table, path in csv_sources.items():\n",
    "    print(f\"  {table}: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a1cf91-97b2-4980-9e50-71b8931eb64c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType, BooleanType\n",
    "\n",
    "# Define explicit schema for claims (prevents inference issues)\n",
    "claims_schema = StructType([\n",
    "    StructField(\"claim_no\", StringType(), True),\n",
    "    StructField(\"policy_no\", StringType(), True),\n",
    "    StructField(\"claim_date\", DateType(), True),\n",
    "    StructField(\"months_as_customer\", IntegerType(), True),\n",
    "    StructField(\"injury\", IntegerType(), True),\n",
    "    StructField(\"property\", IntegerType(), True),\n",
    "    StructField(\"vehicle\", IntegerType(), True),\n",
    "    StructField(\"total\", IntegerType(), True),\n",
    "    StructField(\"collision_type\", StringType(), True),\n",
    "    StructField(\"number_of_vehicles_involved\", IntegerType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"insured_relationship\", StringType(), True),\n",
    "    StructField(\"license_issue_date\", DateType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"severity\", StringType(), True),\n",
    "    StructField(\"number_of_witnesses\", IntegerType(), True),\n",
    "    StructField(\"suspicious_activity\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "for table_name, file_path in csv_sources.items():\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {table_name}\")\n",
    "    print(f\"Source: {file_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Drop existing table if it exists (clean slate)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(f\"Dropped existing table (if any): {table_name}\")\n",
    "    \n",
    "    # Read CSV with explicit schema (for claims) or infer for others\n",
    "    if table_name == \"claims_raw\":\n",
    "        # Use explicit schema for claims\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .schema(claims_schema) \\\n",
    "            .csv(file_path)\n",
    "    else:\n",
    "        # Infer schema for other tables (simpler for now)\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"inferSchema\", True) \\\n",
    "            .csv(file_path)\n",
    "    \n",
    "    # Show schema\n",
    "    print(\"\\nSchema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Preview data\n",
    "    print(\"\\nSample data (first 3 rows):\")\n",
    "    df.show(3, truncate=False)\n",
    "    \n",
    "    # Write to Delta table (no mergeSchema needed - fresh table)\n",
    "    df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    \n",
    "    # Get row count\n",
    "    row_count = spark.table(table_name).count()\n",
    "    \n",
    "    print(f\"âœ… Created table: {table_name}\")\n",
    "    print(f\"   Rows: {row_count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ CSV INGESTION COMPLETE!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728c4a2c-5b27-480a-9fe3-bb24f406fcf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List tables\n",
    "print(\"Bronze tables created:\")\n",
    "tables = spark.sql(\"SHOW TABLES IN smart_claims_dev.bronze\")\n",
    "tables.show(truncate=False)\n",
    "\n",
    "# Row counts\n",
    "print(\"\\nRow counts:\")\n",
    "for table in [\"claims_raw\", \"customers_raw\", \"policies_raw\"]:\n",
    "    count = spark.table(table).count()\n",
    "    print(f\"  {table}: {count:,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c70a6e-a016-4439-9585-666eafe451a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze_batch_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
