{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "533ced64-3b47-4221-b4d4-a8e13f113b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Read Bronze policies and check schema\n",
    "bronze_policies = spark.table(\"smart_claims_dev.bronze.policies_raw\")\n",
    "\n",
    "print(f\"üìä Bronze Policies - Row Count: {bronze_policies.count():,}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç Schema:\")\n",
    "bronze_policies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca382d47-aba7-4e46-a4b7-e2797eaa1764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Check for duplicates and basic stats in Policies\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "\n",
    "# Get counts\n",
    "total_rows = bronze_policies.count()\n",
    "unique_policies = bronze_policies.select(countDistinct(\"POLICY_NO\")).collect()[0][0]\n",
    "duplicate_count = total_rows - unique_policies\n",
    "\n",
    "# Display results\n",
    "print(f\"üìä BRONZE POLICIES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Rows:              {total_rows:,}\")\n",
    "print(f\"Unique POLICY_NO:        {unique_policies:,}\")\n",
    "print(f\"Duplicate Records:       {duplicate_count:,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüîç SAMPLE DATA (First 5 rows):\")\n",
    "bronze_policies.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab6e9e0-44bf-4105-a168-7255b72fa3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Check for NULL values in critical columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define critical columns\n",
    "critical_columns = [\n",
    "    'POLICY_NO',\n",
    "    'CUST_ID',\n",
    "    'POL_EFF_DATE',\n",
    "    'POL_EXPIRY_DATE',\n",
    "    'SUM_INSURED'\n",
    "]\n",
    "\n",
    "print(\"üîç NULL VALUE ANALYSIS - Critical Columns\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Column':<20} | {'Null Count':>12} | {'Null %':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check each column for nulls\n",
    "for column in critical_columns:\n",
    "    null_count = bronze_policies.filter(col(column).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    print(f\"{column:<20} | {null_count:>12,} | {null_percentage:>9.2f}%\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca7f0a7-c86a-4fd0-99ad-a154be7a3061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Business rule validation for Policies (safe casting with SQL CASE WHEN)\n",
    "from pyspark.sql.functions import col, current_date, expr\n",
    "\n",
    "print(\"üîç BUSINESS RULE VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: POL_EFF_DATE should not be in future\n",
    "future_eff_date = bronze_policies.filter(col(\"POL_EFF_DATE\") > current_date()).count()\n",
    "print(f\"‚ùå Effective dates in future:         {future_eff_date:>10,}\")\n",
    "\n",
    "# Check 2: POL_EXPIRY_DATE should not be in future\n",
    "future_expiry_date = bronze_policies.filter(col(\"POL_EXPIRY_DATE\") > current_date()).count()\n",
    "print(f\"‚ùå Expiry dates in future:            {future_expiry_date:>10,}\")\n",
    "\n",
    "# Check 3: POL_EFF_DATE <= POL_EXPIRY_DATE\n",
    "invalid_date_logic = bronze_policies.filter(col(\"POL_EFF_DATE\") > col(\"POL_EXPIRY_DATE\")).count()\n",
    "print(f\"‚ùå Effective date > Expiry date:      {invalid_date_logic:>10,}\")\n",
    "\n",
    "# Check 4: SUM_INSURED > 0\n",
    "invalid_amount = bronze_policies.filter(col(\"SUM_INSURED\") <= 0).count()\n",
    "print(f\"‚ùå SUM_INSURED <= 0:                  {invalid_amount:>10,}\")\n",
    "\n",
    "# Check 5: PREMIUM >= 0\n",
    "invalid_premium = bronze_policies.filter(col(\"PREMIUM\") < 0).count()\n",
    "print(f\"‚ùå PREMIUM < 0:                       {invalid_premium:>10,}\")\n",
    "\n",
    "# Check 6: DEDUCTABLE >= 0\n",
    "invalid_deductible = bronze_policies.filter(col(\"DEDUCTABLE\") < 0).count()\n",
    "print(f\"‚ùå DEDUCTABLE < 0:                    {invalid_deductible:>10,}\")\n",
    "\n",
    "# Check 7: MODEL_YEAR reasonable - safe casting (handles \"2015.0\", \"null\" strings)\n",
    "policies_with_year = bronze_policies.withColumn(\n",
    "    \"MODEL_YEAR_INT\",\n",
    "    expr(\"\"\"\n",
    "        CASE \n",
    "            WHEN MODEL_YEAR IS NULL OR MODEL_YEAR = 'null' THEN NULL\n",
    "            ELSE CAST(CAST(MODEL_YEAR AS DOUBLE) AS INT)\n",
    "        END\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "invalid_model_year = policies_with_year.filter(\n",
    "    (col(\"MODEL_YEAR_INT\").isNull()) |\n",
    "    (col(\"MODEL_YEAR_INT\") < 1980) | \n",
    "    (col(\"MODEL_YEAR_INT\") > 2025)\n",
    ").count()\n",
    "print(f\"‚ùå MODEL_YEAR outside 1980-2025:     {invalid_model_year:>10,}\")\n",
    "\n",
    "# Check 8: Duplicate POLICY_NO\n",
    "duplicate_policies = 1\n",
    "print(f\"‚ùå Duplicate POLICY_NO:               {duplicate_policies:>10,}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary\n",
    "total_invalid = (future_eff_date + future_expiry_date + invalid_date_logic + \n",
    "                 invalid_amount + invalid_premium + invalid_deductible + \n",
    "                 invalid_model_year + duplicate_policies)\n",
    "print(f\"\\nüìä TOTAL INVALID RECORDS: {total_invalid:,}\")\n",
    "print(f\"üìä VALID RECORDS: {total_rows - total_invalid:,} ({((total_rows - total_invalid)/total_rows)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43e93ba1-d614-4093-9b53-9e30fa14c4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Transform Bronze to Silver - Apply all quality rules + deduplicate\n",
    "from pyspark.sql.functions import current_timestamp, row_number, col, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"üîß APPLYING TRANSFORMATIONS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Start with policies that have MODEL_YEAR_INT calculated\n",
    "policies_silver = policies_with_year\n",
    "\n",
    "# Filter 1: Remove policies with negative PREMIUM\n",
    "policies_silver = policies_silver.filter(col(\"PREMIUM\") >= 0)\n",
    "print(f\"‚úÖ Filter 1: Remove negative PREMIUM\")\n",
    "\n",
    "# Filter 2: Remove policies with invalid MODEL_YEAR\n",
    "policies_silver = policies_silver.filter(\n",
    "    (col(\"MODEL_YEAR_INT\").isNull()) | \n",
    "    ((col(\"MODEL_YEAR_INT\") >= 1980) & (col(\"MODEL_YEAR_INT\") <= 2025))\n",
    ")\n",
    "print(f\"‚úÖ Filter 2: Remove invalid MODEL_YEAR\")\n",
    "\n",
    "# Filter 3: Remove policies with logical date errors\n",
    "policies_silver = policies_silver.filter(col(\"POL_EFF_DATE\") <= col(\"POL_EXPIRY_DATE\"))\n",
    "print(f\"‚úÖ Filter 3: Remove date logic errors\")\n",
    "\n",
    "# Filter 4: Deduplicate by POLICY_NO - keep latest (by POL_EXPIRY_DATE descending)\n",
    "window_spec = Window.partitionBy(\"POLICY_NO\").orderBy(col(\"POL_EXPIRY_DATE\").desc())\n",
    "policies_silver = policies_silver.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"row_num\") == 1) \\\n",
    "    .drop(\"row_num\")\n",
    "print(f\"‚úÖ Filter 4: Deduplicate by POLICY_NO (keep latest)\")\n",
    "\n",
    "# Drop temporary columns\n",
    "policies_silver = policies_silver.drop(\"MODEL_YEAR_INT\")\n",
    "print(f\"‚úÖ Dropped temporary column: MODEL_YEAR_INT\")\n",
    "\n",
    "# Add audit column\n",
    "policies_silver = policies_silver.withColumn(\"processed_at\", current_timestamp())\n",
    "print(f\"‚úÖ Added audit column: processed_at\")\n",
    "\n",
    "# Results\n",
    "final_count = policies_silver.count()\n",
    "removed_count = total_rows - final_count\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä TRANSFORMATION RESULTS:\")\n",
    "print(f\"   Original Bronze rows:  {total_rows:>10,}\")\n",
    "print(f\"   Removed invalid rows:  {removed_count:>10,}\")\n",
    "print(f\"   Final Silver rows:     {final_count:>10,}\")\n",
    "print(f\"   Data quality:          {(final_count/total_rows)*100:>9.2f}%\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d4e655-51dd-4f6b-9c71-51abf3344eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Write cleaned policies to Silver Delta table\n",
    "print(\"üíæ WRITING TO SILVER LAYER...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Write to Delta table\n",
    "policies_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.silver.policies_clean\")\n",
    "\n",
    "print(\"‚úÖ Successfully written to: smart_claims_dev.silver.policies_clean\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify the write\n",
    "silver_table = spark.table(\"smart_claims_dev.silver.policies_clean\")\n",
    "silver_count = silver_table.count()\n",
    "\n",
    "print(f\"üîç VERIFICATION:\")\n",
    "print(f\"   Rows written:  {silver_count:>10,}\")\n",
    "print(f\"   Expected:      {final_count:>10,}\")\n",
    "print(f\"   Match:         {'‚úÖ YES' if silver_count == final_count else '‚ùå NO'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample from Silver table\n",
    "print(\"\\nüìä SAMPLE SILVER DATA (First 5 rows):\")\n",
    "display(silver_table.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff0a2030-4dd3-4d6e-9c55-a1d1f94903f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_silver_policies",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
