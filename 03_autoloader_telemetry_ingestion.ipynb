{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3597bd6-add0-4edc-aca8-98aa0d2f6519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: CONFIGURATION (CORRECTED PATHS)\n",
    "# ============================================\n",
    "\n",
    "# Set catalog and schema\n",
    "spark.sql(\"USE CATALOG smart_claims_dev\")\n",
    "spark.sql(\"USE SCHEMA bronze\")\n",
    "\n",
    "print(f\"Current Catalog: {spark.catalog.currentCatalog()}\")\n",
    "print(f\"Current Database: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# File paths - Use SAME volume (raw_files) for both data and checkpoints\n",
    "source_path = \"/Volumes/smart_claims_dev/landing/raw_files/kinesis_landing/\"\n",
    "checkpoint_path = \"/Volumes/smart_claims_dev/landing/raw_files/_checkpoints/telemetry\"  # âœ… Inside raw_files volume\n",
    "table_name = \"telematics_raw\"\n",
    "\n",
    "print(\"\\nâœ… Configuration loaded\")\n",
    "print(f\"   Source path: {source_path}\")\n",
    "print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "print(f\"   Target table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "196c2610-c027-4d45-9703-6115aedbab85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1B: CREATE CHECKPOINT DIRECTORY\n",
    "# ============================================\n",
    "\n",
    "# Create checkpoint directory inside raw_files volume\n",
    "checkpoint_dir = \"/Volumes/smart_claims_dev/landing/raw_files/_checkpoints\"\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(checkpoint_dir)\n",
    "    print(f\"âœ… Checkpoint directory already exists: {checkpoint_dir}\")\n",
    "except Exception:\n",
    "    print(f\"Creating checkpoint directory: {checkpoint_dir}\")\n",
    "    dbutils.fs.mkdirs(checkpoint_dir)\n",
    "    print(f\"âœ… Directory created successfully\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nDirectory created at:\")\n",
    "display(dbutils.fs.ls(\"/Volumes/smart_claims_dev/landing/raw_files/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24d509a-f184-405e-a220-86a38d398085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: DEFINE TELEMETRY SCHEMA\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Schema matching generator output\n",
    "telemetry_schema = StructType([\n",
    "    StructField(\"vehicle_id\", StringType(), True),\n",
    "    StructField(\"speed_mph\", DoubleType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"acceleration_mps2\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "print(\"âœ… Schema defined:\")\n",
    "for field in telemetry_schema.fields:\n",
    "    print(f\"   {field.name}: {field.dataType}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c55595-6f6a-49a3-9a04-aa0f5f0bab0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: READ FILES WITH AUTO LOADER\n",
    "# ============================================\n",
    "\n",
    "# Read telemetry files using Auto Loader\n",
    "telemetry_stream = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}/schema\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(telemetry_schema) \\\n",
    "    .load(source_path)\n",
    "\n",
    "print(\"âœ… Auto Loader configured\")\n",
    "print(\"\\nStream schema:\")\n",
    "telemetry_stream.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd5decc-2d3b-4767-ae31-39eae9352881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: WRITE STREAM TO DELTA TABLE\n",
    "# ============================================\n",
    "\n",
    "# Write to Bronze Delta table\n",
    "query = telemetry_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .table(table_name)\n",
    "\n",
    "print(\"âœ… Auto Loader processing started!\")\n",
    "print(f\"\\nQuery ID: {query.id}\")\n",
    "print(\"\\nðŸ”„ Processing files...\")\n",
    "print(\"   (This will process all 10 CSV files)\")\n",
    "\n",
    "# Wait for completion (availableNow mode stops automatically)\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"\\nâœ… Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c3ffe7-f931-44c9-952d-98f5f4331494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: VERIFY INGESTION\n",
    "# ============================================\n",
    "\n",
    "# Count rows\n",
    "total_rows = spark.table(table_name).count()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ‰ AUTO LOADER INGESTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total rows ingested: {total_rows:,}\")\n",
    "print(f\"Expected rows: 5,000 (10 batches Ã— 500 events)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample telemetry data:\")\n",
    "spark.table(table_name).show(5, truncate=False)\n",
    "\n",
    "# Show vehicle distribution\n",
    "print(\"\\nVehicle distribution (top 10):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT vehicle_id, COUNT(*) as event_count\n",
    "    FROM {table_name}\n",
    "    GROUP BY vehicle_id\n",
    "    ORDER BY event_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_autoloader_telemetry_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
