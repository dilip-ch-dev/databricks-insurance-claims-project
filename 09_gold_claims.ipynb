{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0e325b-1c12-44ae-a878-013072b72d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Read Silver Claims and start Gold aggregations\n",
    "from pyspark.sql.functions import col, count, sum, avg, current_timestamp, year, month, quarter\n",
    "\n",
    "print(\"ðŸ“Š GOLD LAYER: CLAIMS AGGREGATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read cleaned Silver claims\n",
    "silver_claims = spark.table(\"smart_claims_dev.silver.claims_clean\")\n",
    "\n",
    "print(f\"ðŸ“ˆ Silver Claims Input: {silver_claims.count():,} rows\")\n",
    "print(\"\\nðŸ” Schema:\")\n",
    "silver_claims.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16051997-6148-466b-b6a0-664ee7ffdc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Claims aggregation by date and severity\n",
    "from pyspark.sql.functions import col, count, sum, avg, year, month, date_format, current_timestamp\n",
    "\n",
    "print(\"ðŸ“Š GOLD: CLAIMS BY DATE & SEVERITY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read Silver claims\n",
    "silver_claims = spark.table(\"smart_claims_dev.silver.claims_clean\")\n",
    "\n",
    "# Aggregate claims by date and severity\n",
    "claims_by_date_severity = silver_claims.groupBy(\n",
    "    date_format(col(\"claim_date\"), \"yyyy-MM\").alias(\"claim_month\"),\n",
    "    col(\"severity\")\n",
    ").agg(\n",
    "    count(\"claim_no\").alias(\"claim_count\"),\n",
    "    avg(col(\"total\")).alias(\"avg_payout\"),\n",
    "    sum(col(\"total\")).alias(\"total_payout\"),\n",
    "    avg(col(\"age\")).alias(\"avg_customer_age\"),\n",
    "    sum(col(\"injury\")).alias(\"total_injury_claims\"),\n",
    "    sum(col(\"property\")).alias(\"total_property_claims\"),\n",
    "    sum(col(\"vehicle\")).alias(\"total_vehicle_claims\")\n",
    ").withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "print(f\"âœ… Created: claims_by_date_severity\")\n",
    "print(f\"   Rows: {claims_by_date_severity.count():,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ” SAMPLE DATA:\")\n",
    "claims_by_date_severity.show(10, truncate=False)\n",
    "\n",
    "# Write to Gold layer\n",
    "claims_by_date_severity.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.gold.claims_by_date_severity\")\n",
    "\n",
    "print(\"\\nâœ… Written to: smart_claims_dev.gold.claims_by_date_severity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864d8347-6db6-465a-ac84-78dad0dfefa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Claims aggregation by collision type\n",
    "from pyspark.sql.functions import col, count, sum, avg, current_timestamp\n",
    "\n",
    "print(\"ðŸ“Š GOLD: CLAIMS BY COLLISION TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read Silver claims\n",
    "silver_claims = spark.table(\"smart_claims_dev.silver.claims_clean\")\n",
    "\n",
    "# Aggregate by collision type\n",
    "claims_by_collision = silver_claims.groupBy(\n",
    "    col(\"collision_type\")\n",
    ").agg(\n",
    "    count(\"claim_no\").alias(\"claim_count\"),\n",
    "    avg(col(\"total\")).alias(\"avg_payout\"),\n",
    "    sum(col(\"total\")).alias(\"total_payout\"),\n",
    "    avg(col(\"number_of_vehicles_involved\")).alias(\"avg_vehicles\"),\n",
    "    sum(col(\"number_of_witnesses\")).alias(\"total_witnesses\"),\n",
    "    sum(col(\"suspicious_activity\").cast(\"int\")).alias(\"suspicious_claims\")\n",
    ").withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "print(f\"âœ… Created: claims_by_collision\")\n",
    "print(f\"   Rows: {claims_by_collision.count():,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ” SAMPLE DATA:\")\n",
    "claims_by_collision.show(10, truncate=False)\n",
    "\n",
    "# Write to Gold layer\n",
    "claims_by_collision.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.gold.claims_by_collision_type\")\n",
    "\n",
    "print(\"\\nâœ… Written to: smart_claims_dev.gold.claims_by_collision_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea824ae-580f-4d8f-b673-eecc5ee53c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Customer behavior metrics\n",
    "from pyspark.sql.functions import col, count, sum, avg, current_timestamp\n",
    "\n",
    "print(\"ðŸ“Š GOLD: CUSTOMER CLAIMS METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read Silver claims\n",
    "silver_claims = spark.table(\"smart_claims_dev.silver.claims_clean\")\n",
    "\n",
    "# Aggregate by customer\n",
    "customer_metrics = silver_claims.groupBy(\n",
    "    col(\"policy_no\").alias(\"customer_id\")\n",
    ").agg(\n",
    "    count(\"claim_no\").alias(\"total_claims\"),\n",
    "    sum(col(\"total\")).alias(\"total_payout\"),\n",
    "    avg(col(\"total\")).alias(\"avg_claim_amount\"),\n",
    "    sum(col(\"injury\")).alias(\"injury_claims\"),\n",
    "    sum(col(\"property\")).alias(\"property_claims\"),\n",
    "    sum(col(\"vehicle\")).alias(\"vehicle_claims\"),\n",
    "    sum(col(\"suspicious_activity\").cast(\"int\")).alias(\"suspicious_claims\")\n",
    ").withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "count_before = customer_metrics.count()\n",
    "print(f\"âœ… Aggregated: {count_before:,} customers\")\n",
    "\n",
    "# WRITE\n",
    "customer_metrics.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.gold.customer_metrics\")\n",
    "\n",
    "# VERIFY\n",
    "count_after = spark.table(\"smart_claims_dev.gold.customer_metrics\").count()\n",
    "print(f\"âœ… Written and verified: {count_after:,} rows in smart_claims_dev.gold.customer_metrics\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74642ece-74b4-4e16-92e5-6ffe6229839e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Telematics driver behavior scoring\n",
    "from pyspark.sql.functions import col, count, sum, avg, max, current_timestamp, when\n",
    "\n",
    "print(\"ðŸ“Š GOLD: TELEMATICS DRIVER SCORING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read Silver telematics\n",
    "silver_telematics = spark.table(\"smart_claims_dev.silver.telematics_clean\")\n",
    "\n",
    "# Classify risky driving events\n",
    "telematics_scored = silver_telematics.withColumn(\n",
    "    \"high_speed_event\",\n",
    "    when(col(\"speed_mph\") > 80, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"harsh_acceleration_event\",\n",
    "    when(col(\"acceleration_mps2\") > 5, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"harsh_braking_event\",\n",
    "    when(col(\"acceleration_mps2\") < -5, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Aggregate by vehicle\n",
    "driver_risk_scores = telematics_scored.groupBy(\n",
    "    col(\"vehicle_id\").alias(\"vehicle_id\")\n",
    ").agg(\n",
    "    count(\"timestamp\").alias(\"total_events\"),\n",
    "    sum(col(\"high_speed_event\")).alias(\"high_speed_count\"),\n",
    "    sum(col(\"harsh_acceleration_event\")).alias(\"harsh_accel_count\"),\n",
    "    sum(col(\"harsh_braking_event\")).alias(\"harsh_brake_count\"),\n",
    "    avg(col(\"speed_mph\")).alias(\"avg_speed\"),\n",
    "    max(col(\"speed_mph\")).alias(\"max_speed\"),\n",
    "    avg(col(\"acceleration_mps2\")).alias(\"avg_acceleration\")\n",
    ").withColumn(\n",
    "    \"risk_score\",\n",
    "    (col(\"high_speed_count\") * 0.3 + \n",
    "     col(\"harsh_accel_count\") * 0.2 + \n",
    "     col(\"harsh_brake_count\") * 0.2)\n",
    ").withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "print(f\"âœ… Created: driver_risk_scores\")\n",
    "print(f\"   Rows: {driver_risk_scores.count():,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ” SAMPLE DATA (Top 10 by risk score):\")\n",
    "driver_risk_scores.orderBy(col(\"risk_score\").desc()).show(10, truncate=False)\n",
    "\n",
    "# Write to Gold layer\n",
    "driver_risk_scores.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.gold.driver_risk_scores\")\n",
    "\n",
    "print(\"\\nâœ… Written to: smart_claims_dev.gold.driver_risk_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7086703e-f85b-4af1-a5b3-6328a396cf6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6A: Inspect Silver Policies\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"ðŸ” DEBUGGING: Silver Policies Data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "silver_policies = spark.table(\"smart_claims_dev.silver.policies_clean\")\n",
    "\n",
    "# Check schema\n",
    "print(\"ðŸ“Š Schema:\")\n",
    "silver_policies.printSchema()\n",
    "\n",
    "# Show raw values of problematic columns\n",
    "print(\"\\nðŸ“‹ Sample SUM_INSURED values (raw):\")\n",
    "silver_policies.select(\"POLICY_NO\", \"SUM_INSURED\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“‹ Sample PREMIUM values (raw):\")\n",
    "silver_policies.select(\"POLICY_NO\", \"PREMIUM\").show(10, truncate=False)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nðŸ“Š Column types:\")\n",
    "print(f\"SUM_INSURED type: {silver_policies.schema['SUM_INSURED'].dataType}\")\n",
    "print(f\"PREMIUM type: {silver_policies.schema['PREMIUM'].dataType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04034e12-ed28-4194-89c8-c12bd51962b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: ML feature engineering - NO JOINS, just claims data\n",
    "from pyspark.sql.functions import col, when, current_timestamp\n",
    "\n",
    "print(\"ðŸ“Š GOLD: ML FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read ONLY Silver claims\n",
    "silver_claims = spark.table(\"smart_claims_dev.silver.claims_clean\")\n",
    "\n",
    "# Feature engineering - NO JOINS\n",
    "ml_features_engineered = silver_claims.select(\n",
    "    col(\"claim_no\").alias(\"claim_id\"),\n",
    "    col(\"policy_no\"),\n",
    "    col(\"age\"),\n",
    "    col(\"total\").alias(\"claim_amount\"),\n",
    "    col(\"months_as_customer\"),\n",
    "    col(\"number_of_vehicles_involved\"),\n",
    "    col(\"number_of_witnesses\"),\n",
    "    col(\"suspicious_activity\").cast(\"int\").alias(\"suspicious_flag\"),\n",
    "    \n",
    "    # Fraud indicators\n",
    "    when(col(\"suspicious_activity\") == True, 1).otherwise(0).alias(\"fraud_indicator\"),\n",
    "    when(col(\"number_of_witnesses\") == 0, 1).otherwise(0).alias(\"no_witnesses_flag\"),\n",
    "    when(col(\"months_as_customer\") < 6, 1).otherwise(0).alias(\"new_customer_flag\"),\n",
    "    \n",
    "    # Claim severity features\n",
    "    when(col(\"severity\") == \"Total Loss\", 1).otherwise(0).alias(\"total_loss_flag\"),\n",
    "    when(col(\"severity\") == \"Major\", 1).otherwise(0).alias(\"major_damage_flag\"),\n",
    "    when(col(\"number_of_vehicles_involved\") > 2, 1).otherwise(0).alias(\"multi_vehicle_flag\")\n",
    ").withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "print(f\"âœ… Created: ml_features_engineered\")\n",
    "print(f\"   Rows: {ml_features_engineered.count():,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ” SAMPLE ML FEATURES (First 5 rows):\")\n",
    "ml_features_engineered.show(5, truncate=False)\n",
    "\n",
    "# Write to Gold layer\n",
    "ml_features_engineered.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.gold.ml_features\")\n",
    "\n",
    "print(\"\\nâœ… Written to: smart_claims_dev.gold.ml_features\")\n",
    "\n",
    "# Show data types\n",
    "print(\"\\nðŸ“Š ML FEATURES SCHEMA:\")\n",
    "ml_features_engineered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb50428f-513f-458c-9aa9-b52d29dc0c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Gold Layer Summary - All tables complete\n",
    "from pyspark.sql.functions import col, count\n",
    "import time\n",
    "\n",
    "print(\"ðŸ† GOLD LAYER COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "time.sleep(1)  # delay for table availability\n",
    "\n",
    "# Read all Gold tables\n",
    "gold_claims_date_severity = spark.table(\"smart_claims_dev.gold.claims_by_date_severity\")\n",
    "gold_claims_collision = spark.table(\"smart_claims_dev.gold.claims_by_collision_type\")\n",
    "gold_customer_metrics = spark.table(\"smart_claims_dev.gold.customer_metrics\")\n",
    "gold_driver_scores = spark.table(\"smart_claims_dev.gold.driver_risk_scores\")\n",
    "gold_ml_features = spark.table(\"smart_claims_dev.gold.ml_features\")\n",
    "\n",
    "print(\"\\nðŸ“Š GOLD LAYER TABLE SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… claims_by_date_severity:    {gold_claims_date_severity.count():>10,} rows\")\n",
    "print(f\"âœ… claims_by_collision_type:   {gold_claims_collision.count():>10,} rows\")\n",
    "print(f\"âœ… customer_metrics:           {gold_customer_metrics.count():>10,} rows\")\n",
    "print(f\"âœ… driver_risk_scores:         {gold_driver_scores.count():>10,} rows\")\n",
    "print(f\"âœ… ml_features:                {gold_ml_features.count():>10,} rows\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“ˆ MEDALLION ARCHITECTURE COMPLETE:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Bronze Layer:  37,289 raw records\")\n",
    "print(\"Silver Layer:  31,504 validated records (84.5% quality)\")\n",
    "print(\"Gold Layer:    5 analytics & ML-ready tables\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸŽ¯ PROJECT STATUS: PRODUCTION-READY END-TO-END PIPELINE âœ…\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09_gold_claims",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
