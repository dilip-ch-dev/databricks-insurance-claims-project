{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e684351f-197e-40dc-bfab-678332d974f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Set catalog and schema\n",
    "spark.sql(\"USE CATALOG smart_claims_dev\")\n",
    "spark.sql(\"USE SCHEMA bronze\")\n",
    "\n",
    "print(f\"Current Catalog: {spark.catalog.currentCatalog()}\")\n",
    "print(f\"Current Database: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# AWS Configuration\n",
    "# TODO: Move to Databricks Secrets for production\n",
    "# For now, using plaintext (replace with your actual credentials)\n",
    "# Cell 1: Import Configuration\n",
    "from config import AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION\n",
    "\n",
    "# NOTE: This notebook demonstrates Kinesis streaming ingestion\n",
    "# ⚠️ REQUIRES: Databricks paid tier (Standard/Premium/Enterprise)\n",
    "# Free Edition limitation: Streaming not supported on shared clusters\n",
    "# Current implementation uses Auto Loader instead (see notebook 03)\n",
    "\n",
    "KINESIS_STREAM_NAME = \"insurance-telemetry-stream\"\n",
    "\n",
    "print(\"\\n✅ Configuration loaded\")\n",
    "print(f\"   Region: {AWS_REGION}\")\n",
    "print(f\"   Stream: {KINESIS_STREAM_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0401b48b-f619-4fcf-8f38-984185fcc69d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: DEFINE TELEMETRY SCHEMA\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Schema for incoming JSON telemetry events\n",
    "telemetry_schema = StructType([\n",
    "    StructField(\"vehicle_id\", StringType(), False),      # Required: Vehicle identifier\n",
    "    StructField(\"speed_mph\", DoubleType(), True),        # Optional: Speed in miles per hour\n",
    "    StructField(\"latitude\", DoubleType(), True),         # Optional: GPS latitude\n",
    "    StructField(\"longitude\", DoubleType(), True),        # Optional: GPS longitude\n",
    "    StructField(\"acceleration_mps2\", DoubleType(), True), # Optional: Acceleration\n",
    "    StructField(\"timestamp\", TimestampType(), True)      # Optional: Event timestamp\n",
    "])\n",
    "\n",
    "print(\"✅ Telemetry schema defined:\")\n",
    "print(telemetry_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68845d69-56a1-451a-bb1d-167e0dc6e0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: READ FROM KINESIS STREAM\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# Read from Kinesis stream\n",
    "kinesis_stream = spark.readStream \\\n",
    "    .format(\"kinesis\") \\\n",
    "    .option(\"streamName\", KINESIS_STREAM_NAME) \\\n",
    "    .option(\"region\", AWS_REGION) \\\n",
    "    .option(\"awsAccessKey\", AWS_ACCESS_KEY) \\\n",
    "    .option(\"awsSecretKey\", AWS_SECRET_KEY) \\\n",
    "    .option(\"startingPosition\", \"LATEST\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"✅ Connected to Kinesis stream\")\n",
    "print(\"\\nRaw stream schema (before parsing):\")\n",
    "kinesis_stream.printSchema()\n",
    "\n",
    "# Parse JSON data from Kinesis\n",
    "# Kinesis returns: data (binary), partitionKey, sequenceNumber, approximateArrivalTimestamp\n",
    "telemetry_df = kinesis_stream \\\n",
    "    .selectExpr(\"CAST(data AS STRING) as json_data\") \\\n",
    "    .select(from_json(col(\"json_data\"), telemetry_schema).alias(\"telemetry\")) \\\n",
    "    .select(\"telemetry.*\")\n",
    "\n",
    "print(\"\\n✅ JSON parsed successfully\")\n",
    "print(\"\\nParsed telemetry schema:\")\n",
    "telemetry_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1568461-ec74-400a-a465-ccc0c9b0b210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_kinesis_streaming_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
