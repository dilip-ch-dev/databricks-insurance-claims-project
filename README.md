# ğŸš— Databricks Insurance Claims Automation Project

**End-to-end data engineering and ML project for learning Databricks Lakehouse Platform**

[![Databricks](https://img.shields.io/badge/Databricks-FF3621?logo=databricks&logoColor=white)](https://databricks.com)
[![AWS](https://img.shields.io/badge/AWS-232F3E?logo=amazon-aws&logoColor=white)](https://aws.amazon.com)
[![Python](https://img.shields.io/badge/Python-3.13-blue?logo=python)](https://python.org)

---

## ğŸ“‹ Project Overview

Automated car insurance claims processing system demonstrating:
- **Unity Catalog governance** (medallion architecture)
- **Multi-source data ingestion** (SQL Server CDC, Kinesis streams, S3 Auto Loader)
- **ML-based damage classification** (Computer vision with MLflow)
- **Real-time dashboards** (Databricks SQL + AI/BI)
- **Web application** (Databricks Apps for claims portal)

### Business Use Case
**Problem**: Manual insurance claims processing is slow, error-prone, and expensive.

**Solution**: Automated claims validation system that:
- âœ… Verifies driver speed from telematics data
- âœ… Checks policy eligibility for refunds
- âœ… Classifies car damage severity using ML image analysis
- âœ… Provides real-time analytics for claim approval decisions

---

## ğŸ—ï¸ Architecture

### Medallion Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL Server â”‚ â”‚ Kinesis â”‚ â”‚ S3 Images â”‚
â”‚ (CDC) â”‚ â”‚ (Streaming) â”‚ â”‚(Auto Loader)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ LANDING â”‚ (Raw files/volumes)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ BRONZE â”‚ (Raw Delta tables)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚ Data Quality + Cleaning
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ SILVER â”‚ (Validated data)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚ Aggregations + ML
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ GOLD â”‚ (Analytics-ready)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Dashboardsâ”‚ â”‚Claims Portalâ”‚
â”‚ + Genie â”‚ â”‚ (App) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

### Unity Catalog Structure

smart_claims_dev (Catalog)
â”œâ”€â”€ landing - Raw file ingestion
â”œâ”€â”€ bronze - Raw Delta tables
â”œâ”€â”€ silver - Cleaned & validated
â””â”€â”€ gold - Analytics aggregates
---

## ğŸ› ï¸ Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Cloud Platform** | AWS (Free Tier) | S3, Kinesis, EC2 |
| **Data Platform** | Databricks Free Edition | Lakehouse, Unity Catalog |
| **Storage** | Delta Lake | ACID transactions, time travel |
| **Orchestration** | LakeFlow Pipelines | ETL automation |
| **ML** | MLflow + Mosaic AI | Model tracking, serving |
| **Database** | SQL Server (Docker) | Source data with CDC |
| **Version Control** | Git + GitHub | Code management |

---

## ğŸ“ Project Structure

databricks-insurance-claims-project/
â”œâ”€â”€ sql_queries.sql # Unity Catalog setup + transformations
â”œâ”€â”€ notebooks/ # (Future) Python notebooks
â”œâ”€â”€ pipelines/ # (Future) LakeFlow pipeline configs
â”œâ”€â”€ data/ # (Future) Sample datasets
â””â”€â”€ docs/ # (Future) Documentation

---

## ğŸš€ Progress Tracker

### âœ… Completed
- [x] Unity Catalog created (`smart_claims_dev`)
- [x] Medallion architecture schemas (landing/bronze/silver/gold)
- [x] Git integration with GitHub
- [x] Databricks Free Edition workspace setup

### ğŸ”„ In Progress
- [ ] AWS S3 bucket configuration
- [ ] SQL Server Docker setup with CDC
- [ ] Kinesis data stream ingestion

### ğŸ“‹ Planned
- [ ] LakeFlow Connect (SQL Server CDC)
- [ ] Auto Loader (S3 file ingestion)
- [ ] Bronze â†’ Silver transformations
- [ ] Silver â†’ Gold aggregations
- [ ] ML model training (car damage classifier)
- [ ] Databricks SQL dashboards
- [ ] Genie AI/BI interface
- [ ] Claims submission portal (Databricks Apps)

---

## ğŸ“ Learning Outcomes

**Data Engineering**:
- Unity Catalog governance
- Medallion architecture pattern
- Change Data Capture (CDC)
- Stream processing
- Delta Lake optimization

**Machine Learning**:
- Computer vision (ResNet)
- MLflow lifecycle management
- Model serving endpoints

**Cloud & DevOps**:
- AWS integration (S3/Kinesis)
- Docker containerization
- Git version control

---

## ğŸ“ Documentation

- **Setup Guide**: See `sql_queries.sql` for step-by-step Unity Catalog setup
- **Tutorial Reference**: Based on [Thomas Hass - Databricks Zero to Hero](https://youtu.be/gFAnlTM-3Zo)
- **Code Repository**: [GitHub - datamyselfai/databricks-zero-to-hero-course](https://github.com/datamyselfai/databricks-zero-to-hero-course)

---

## ğŸ‘¤ Author

**[Your Name]**  
ğŸ“§ Email: dilip77950@gmail.com  
ğŸ’¼ LinkedIn: [Add your LinkedIn profile]  
ğŸ¯ Goal: Master Databricks for data engineering roles

---

## ğŸ“„ License

MIT License - Free to use for learning purposes

---

**â­ Star this repo if you find it helpful for your Databricks learning journey!**

*Last Updated: November 10, 2025*

