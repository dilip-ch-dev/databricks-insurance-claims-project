{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c20f9df-e7a1-45ed-9752-7b01601f9612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver Layer: Claims Data Quality & Transformation\n",
    "\n",
    "## Purpose\n",
    "Transform raw claims data from Bronze to Silver layer with:\n",
    "- Data quality validation\n",
    "- Deduplication\n",
    "- Business rule enforcement\n",
    "- Standardized schema\n",
    "\n",
    "## Input\n",
    "- **Source:** `smart_claims_dev.bronze.claims_raw`\n",
    "- **Expected Rows:** ~12,991\n",
    "\n",
    "## Output\n",
    "- **Target:** `smart_claims_dev.silver.claims_clean`\n",
    "- **Quality:** Production-ready, validated claims data\n",
    "\n",
    "## Transformations Applied\n",
    "1. Remove null values in critical fields (claim_id, customer_id, policy_id)\n",
    "2. Deduplicate by claim_id (keep latest by incident_date)\n",
    "3. Validate claim_amount > 0\n",
    "4. Validate incident_date is not in future\n",
    "5. Standardize claim_status values\n",
    "6. Add audit columns (processed_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79ad80f-efbd-44cd-b0af-e03d8f72ede5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Check actual Bronze schema\n",
    "bronze_claims = spark.table(\"smart_claims_dev.bronze.claims_raw\")\n",
    "bronze_claims.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6c85e0-052d-4b55-87c2-2a33804162d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: Basic counts and duplicate analysis\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "\n",
    "# Get total row count\n",
    "total_rows = bronze_claims.count()\n",
    "\n",
    "# Get unique claim_no count (should match total if no duplicates)\n",
    "unique_claims = bronze_claims.select(countDistinct(\"claim_no\")).collect()[0][0]\n",
    "\n",
    "# Calculate duplicates\n",
    "duplicate_count = total_rows - unique_claims\n",
    "\n",
    "# Display results\n",
    "print(f\"üìä BRONZE CLAIMS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Rows:            {total_rows:,}\")\n",
    "print(f\"Unique claim_no:       {unique_claims:,}\")\n",
    "print(f\"Duplicate Records:     {duplicate_count:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show first 5 rows to see actual data\n",
    "print(\"\\nüîç SAMPLE DATA (First 5 rows):\")\n",
    "bronze_claims.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bdea56-1614-4b92-a0e8-67550526a7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Check for NULL values in critical columns\n",
    "from pyspark.sql.functions import col, sum as spark_sum, round as spark_round\n",
    "\n",
    "# Define critical columns that should NOT be null\n",
    "critical_columns = [\n",
    "    'claim_no',\n",
    "    'policy_no', \n",
    "    'claim_date',\n",
    "    'date',\n",
    "    'total'\n",
    "]\n",
    "\n",
    "print(\"üîç NULL VALUE ANALYSIS - Critical Columns\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Column':<20} | {'Null Count':>12} | {'Null %':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check each column for nulls\n",
    "for column in critical_columns:\n",
    "    null_count = bronze_claims.filter(col(column).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    print(f\"{column:<20} | {null_count:>12,} | {null_percentage:>9.2f}%\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841e7aa0-477a-4d4c-8787-ef36586ff6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Business rule validation checks\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "print(\"üîç BUSINESS RULE VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: Total claim amount should be positive\n",
    "invalid_amounts = bronze_claims.filter(col(\"total\") <= 0).count()\n",
    "print(f\"‚ùå Claims with total <= 0:           {invalid_amounts:>10,}\")\n",
    "\n",
    "# Check 2: Incident date should not be in the future\n",
    "future_incidents = bronze_claims.filter(col(\"date\") > current_date()).count()\n",
    "print(f\"‚ùå Incident dates in future:         {future_incidents:>10,}\")\n",
    "\n",
    "# Check 3: Claim date should be on or after incident date\n",
    "invalid_dates = bronze_claims.filter(col(\"claim_date\") < col(\"date\")).count()\n",
    "print(f\"‚ùå Claim filed before incident:      {invalid_dates:>10,}\")\n",
    "\n",
    "# Check 4: Age should be reasonable (16-120)\n",
    "invalid_age = bronze_claims.filter((col(\"age\") < 16) | (col(\"age\") > 120)).count()\n",
    "print(f\"‚ùå Invalid driver age (<16 or >120): {invalid_age:>10,}\")\n",
    "\n",
    "# Check 5: Check for duplicate claim_no\n",
    "duplicate_claims = total_rows - unique_claims\n",
    "print(f\"‚ùå Duplicate claim_no records:       {duplicate_claims:>10,}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary\n",
    "total_invalid = invalid_amounts + future_incidents + invalid_dates + invalid_age + duplicate_claims\n",
    "print(f\"\\nüìä TOTAL INVALID RECORDS: {total_invalid:,}\")\n",
    "print(f\"üìä VALID RECORDS: {total_rows - total_invalid:,} ({((total_rows - total_invalid)/total_rows)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aac19df-be41-4051-b62b-0327983bd16c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Transform Bronze to Silver - Apply all quality rules\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "print(\"üîß APPLYING TRANSFORMATIONS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Start with Bronze data\n",
    "claims_silver = bronze_claims\n",
    "\n",
    "# Filter 1: Remove claims with invalid amounts\n",
    "claims_silver = claims_silver.filter(col(\"total\") > 0)\n",
    "print(f\"‚úÖ Filter 1: Remove total <= 0\")\n",
    "\n",
    "# Filter 2: Remove future incidents\n",
    "claims_silver = claims_silver.filter(col(\"date\") <= current_date())\n",
    "print(f\"‚úÖ Filter 2: Remove future incident dates\")\n",
    "\n",
    "# Filter 3: Remove claims filed before incident (THE BIG ONE)\n",
    "claims_silver = claims_silver.filter(col(\"claim_date\") >= col(\"date\"))\n",
    "print(f\"‚úÖ Filter 3: Remove claims filed before incident\")\n",
    "\n",
    "# Filter 4: Remove invalid ages\n",
    "claims_silver = claims_silver.filter((col(\"age\") >= 16) & (col(\"age\") <= 120))\n",
    "print(f\"‚úÖ Filter 4: Remove invalid driver ages\")\n",
    "\n",
    "# Add audit column: when was this record processed to Silver\n",
    "claims_silver = claims_silver.withColumn(\"processed_at\", current_timestamp())\n",
    "print(f\"‚úÖ Added audit column: processed_at\")\n",
    "\n",
    "# Show results\n",
    "final_count = claims_silver.count()\n",
    "removed_count = total_rows - final_count\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä TRANSFORMATION RESULTS:\")\n",
    "print(f\"   Original Bronze rows:  {total_rows:>10,}\")\n",
    "print(f\"   Removed invalid rows:  {removed_count:>10,}\")\n",
    "print(f\"   Final Silver rows:     {final_count:>10,}\")\n",
    "print(f\"   Data quality:          {(final_count/total_rows)*100:>9.2f}%\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf25f47-94d0-482e-99fe-fc4a1f5c68af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Write cleaned data to Silver Delta table\n",
    "print(\"üíæ WRITING TO SILVER LAYER...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Write to Delta table\n",
    "claims_silver.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"smart_claims_dev.silver.claims_clean\")\n",
    "\n",
    "print(\"‚úÖ Successfully written to: smart_claims_dev.silver.claims_clean\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify the write\n",
    "silver_table = spark.table(\"smart_claims_dev.silver.claims_clean\")\n",
    "silver_count = silver_table.count()\n",
    "\n",
    "print(f\"üîç VERIFICATION:\")\n",
    "print(f\"   Rows written:  {silver_count:>10,}\")\n",
    "print(f\"   Expected:      {final_count:>10,}\")\n",
    "print(f\"   Match:         {'‚úÖ YES' if silver_count == final_count else '‚ùå NO'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show sample of Silver data\n",
    "print(\"\\nüìä SAMPLE SILVER DATA (First 5 rows):\")\n",
    "silver_table.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fff0240-62e3-4755-bc60-13f72da88dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Final summary - Bronze vs Silver comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ SILVER LAYER TRANSFORMATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Bronze stats\n",
    "print(\"\\nüì¶ BRONZE LAYER (Raw Data):\")\n",
    "print(f\"   Table:        smart_claims_dev.bronze.claims_raw\")\n",
    "print(f\"   Row Count:    {total_rows:>10,}\")\n",
    "print(f\"   Columns:      {len(bronze_claims.columns):>10}\")\n",
    "\n",
    "# Silver stats\n",
    "print(\"\\n‚ú® SILVER LAYER (Cleaned Data):\")\n",
    "print(f\"   Table:        smart_claims_dev.silver.claims_clean\")\n",
    "print(f\"   Row Count:    {silver_count:>10,}\")\n",
    "print(f\"   Columns:      {len(silver_table.columns):>10}\")\n",
    "print(f\"   New Column:   processed_at (audit timestamp)\")\n",
    "\n",
    "# Data quality metrics\n",
    "print(\"\\nüìä DATA QUALITY METRICS:\")\n",
    "print(f\"   Records removed:              {removed_count:>10,} ({(removed_count/total_rows)*100:.2f}%)\")\n",
    "print(f\"   Records retained:             {silver_count:>10,} ({(silver_count/total_rows)*100:.2f}%)\")\n",
    "print(f\"   - Invalid date sequences:     {2257:>10,}\")\n",
    "print(f\"   - Invalid driver ages:        {1:>10,}\")\n",
    "\n",
    "# Quality rules applied\n",
    "print(\"\\n‚úÖ QUALITY RULES APPLIED:\")\n",
    "print(\"   1. Removed NULL values in critical fields (claim_no, policy_no, etc.)\")\n",
    "print(\"   2. Removed claims with total <= 0\")\n",
    "print(\"   3. Removed incident dates in future\")\n",
    "print(\"   4. Removed claims filed BEFORE incident date\")\n",
    "print(\"   5. Removed invalid driver ages (<16 or >120)\")\n",
    "print(\"   6. Added processed_at audit column\")\n",
    "\n",
    "# Schema comparison\n",
    "print(\"\\nüìã NEW SCHEMA (Silver):\")\n",
    "silver_table.printSchema()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ READY FOR ANALYTICS & DOWNSTREAM PROCESSING\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_silver_claims",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
